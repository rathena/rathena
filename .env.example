# ============================================
# rAthena-AI-world - Unified Environment Configuration Example
# ============================================
# Copy this file to .env and fill in your actual values.
# DO NOT commit .env to version control!
# All secrets must use placeholders and be set securely in production.

# ============================================
# Database Configuration
# ============================================

# --- DragonFlyDB (Redis protocol, high-speed cache & pub/sub) ---
# DragonFlyDB is used as the primary cache and real-time state backend.
# All credentials must be set via environment variables for security.
DRAGONFLY_HOST=localhost
DRAGONFLY_PORT=6379
DRAGONFLY_DB=0
# ⚠️ SECURITY: DragonflyDB MUST have password authentication enabled in production
# Generate strong password with: openssl rand -base64 32
# Store in secrets vault and reference with vault:// syntax
DRAGONFLY_PASSWORD=<generate_with_openssl_rand_base64_32>
DRAGONFLY_MAX_CONNECTIONS=50
DRAGONFLY_SOCKET_TIMEOUT=5
DRAGONFLY_SOCKET_CONNECT_TIMEOUT=5
# DEVELOPMENT MODE: SSL/TLS disabled for local development
# PRODUCTION: Set DRAGONFLY_SSL=true and DRAGONFLY_SSL_VERIFY=true, ensure certificates are configured
DRAGONFLY_SSL=false
DRAGONFLY_SSL_VERIFY=false

# (Deprecated) Redis variables for backward compatibility. Use DRAGONFLY_* for all new deployments.
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_DB=0
# REDIS_PASSWORD=
# REDIS_MAX_CONNECTIONS=50

# PostgreSQL database name (required)
POSTGRES_DB=ai_world_memory
# PostgreSQL username (required)
POSTGRES_USER=ai_world_user
# PostgreSQL password (required, set a strong password in production)
# ⚠️ SECURITY: Generate a strong password with: openssl rand -base64 32
# Store in secrets vault and reference with vault:// syntax
POSTGRES_PASSWORD=<generate_with_openssl_rand_base64_32>
# PostgreSQL host and port
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
# DEVELOPMENT MODE: Using 'prefer' for local development (will use SSL if available, but not required)
# PRODUCTION: Set POSTGRES_SSLMODE=require to enforce SSL/TLS connections
POSTGRES_SSLMODE=prefer

# ============================================
# LLM Provider API Keys
# ============================================
# At least ONE provider must be configured for AI features.
# Recommended: OpenAI GPT-4 for best results.

# ============================================
# LLM Provider Configuration
# ============================================

# -------- OpenAI --------
# Required for OpenAI GPT usage
OPENAI_API_KEY=sk-your-openai-api-key-here      # Required if using OpenAI. Obtain from https://platform.openai.com/
OPENAI_MODEL=gpt-4                              # Required: Model name (e.g., gpt-4, gpt-3.5-turbo)
OPENAI_API_BASE=https://api.openai.com/v1        # Optional: Override OpenAI API base URL
OPENAI_API_VERSION=2023-07-01                    # Optional: API version (if needed)

# -------- Anthropic Claude --------
# Required for Anthropic Claude usage
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here   # Required if using Anthropic Claude
ANTHROPIC_MODEL=claude-3-opus                          # Required: Model name (e.g., claude-3-opus, claude-2.1)
ANTHROPIC_API_ENDPOINT=https://api.anthropic.com/v1    # Optional: Override Anthropic API endpoint

# -------- Google Gemini --------
# Required for Google Gemini usage
GOOGLE_API_KEY=your-google-api-key-here                # Required if using Google Gemini
GOOGLE_MODEL=gemini-pro                                # Required: Model name (e.g., gemini-pro, gemini-1.5-pro)
GOOGLE_API_ENDPOINT=https://generativelanguage.googleapis.com/v1beta  # Optional: Override Gemini API endpoint

# -------- Azure OpenAI --------
# Required for Azure OpenAI usage
# ⚠️ SECURITY: API keys must be stored in a secrets vault (Azure Key Vault, HashiCorp Vault, etc.)
# Use vault:// syntax to reference secrets from your vault provider
AZURE_OPENAI_API_KEY=vault://secrets/azure-openai-api-key    # Required if using Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://yss.openai.azure.com/  # Required: Azure endpoint URL
AZURE_OPENAI_DEPLOYMENT=gpt-4.1-nano     # Required: Azure deployment/model name
AZURE_OPENAI_MODEL=gpt-4.1-nano                             # Optional: Model name (if different from deployment)
AZURE_OPENAI_API_VERSION=2025-05-01                   # Optional: API version

# -------- DeepSeek --------
# Required for DeepSeek usage
DEEPSEEK_API_KEY=your-deepseek-api-key-here            # Required if using DeepSeek
DEEPSEEK_MODEL=deepseek-chat                           # Required: Model name (e.g., deepseek-chat, deepseek-coder)
DEEPSEEK_API_ENDPOINT=https://api.deepseek.com/v1      # Optional: Override DeepSeek API endpoint

# -------- General LLM Selection --------
DEFAULT_LLM_PROVIDER=azure_openai              # Default: azure_openai (options: azure_openai, openai, anthropic, google, deepseek)
DEFAULT_MODEL=gpt-4.1-nano                            # Default model for LLM requests

# ============================================
# AI Service Configuration
# ============================================
# API key for internal AI service bridge (required, set a strong secret in production)
# ⚠️ SECURITY: Generate with: openssl rand -base64 43
# Store in secrets vault and reference with vault:// syntax
AI_SERVICE_API_KEY=<generate_with_openssl_rand_base64_43>

# API Key Authentication (MUST be enabled in production)
API_KEY_REQUIRED=true

# ============================================
# SSL/TLS Configuration
# ============================================
# DEVELOPMENT MODE: SSL/TLS disabled for local development
# PRODUCTION: Set SSL_ENABLED=true and configure certificate paths below
SSL_ENABLED=false
# SSL certificate file path (generate with: bash scripts/generate-ssl-certs.sh or use Let's Encrypt)
# SSL_CERTFILE=certs/server.crt
# SSL private key file path
# SSL_KEYFILE=certs/server.key
# PRODUCTION REQUIREMENTS:
# 1. Generate valid SSL certificates (self-signed for testing, CA-signed for production)
# 2. Set SSL_ENABLED=true
# 3. Uncomment and configure SSL_CERTFILE and SSL_KEYFILE paths
# 4. Ensure certificate files have correct permissions (readable by server process only)

# Logging level for AI service (optional, default: INFO)
LOG_LEVEL=INFO
# Maximum number of AI worker processes (optional, default: 4)
MAX_WORKERS=4
# Cache time-to-live in seconds (optional, default: 3600)
CACHE_TTL=3600

# ============================================
# Worker Pool / Multi-CPU / P2P Configuration
# ============================================
# Enable worker pool for multi-threaded operation (true/false, optional, default: true)
WORKER_POOL_ENABLED=true
# Number of worker threads (optional, overrides conf/worker_pool.conf)
WORKER_POOL_NUM_THREADS=4
# Minimum threads for dynamic scaling (optional)
WORKER_POOL_MIN_THREADS=2
# Maximum threads for dynamic scaling (optional)
WORKER_POOL_MAX_THREADS=8
# Assignment strategy for worker pool (optional: round_robin, least_loaded, hash)
WORKER_POOL_ASSIGNMENT_STRATEGY=least_loaded
# Enable Prometheus metrics (true/false, optional)
WORKER_POOL_METRICS_ENABLED=true
# Prometheus metrics listen address (optional, default: 0.0.0.0:9100)
WORKER_POOL_METRICS_ADDR=0.0.0.0:9100
# Structured logging level for worker pool (optional: debug, info, warn, error)
WORKER_POOL_LOG_LEVEL=info
# Log file path for worker pool (optional, empty for stdout)
WORKER_POOL_LOG_FILE=
# Verbose logging for worker pool (true/false, optional)
WORKER_POOL_VERBOSE=false
# Enable dynamic scaling of worker pool (true/false, optional)
WORKER_POOL_DYNAMIC_SCALING=true
# CPU affinity for worker pool (optional, comma-separated core indices, empty for auto)
WORKER_POOL_CPU_AFFINITY=

# ============================================
# Performance Tuning
# ============================================
# Dialogue generation temperature (0.0-1.0, higher = more creative, optional, default: 0.7)
DIALOGUE_TEMPERATURE=0.8
# Decision making temperature (optional, default: 0.5)
DECISION_TEMPERATURE=0.5
# Maximum tokens for LLM responses (optional, default: 500)
MAX_TOKENS=500
# Cache expiration in seconds (optional, default: 3600)
CACHE_EXPIRATION=3600

# ============================================
# Feature Flags
# ============================================
# Enable/disable subsystems (true/false, optional)
ENABLE_MEMORY_SYSTEM=true
ENABLE_PERSONALITY_SYSTEM=true
ENABLE_FACTION_SYSTEM=true
ENABLE_ECONOMY_AGENT=true
ENABLE_QUEST_GENERATION=true

# ============================================
# Development/Debug Settings
# ============================================
# Enable debug mode (true/false, optional)
DEBUG_MODE=true
# Enable verbose logging (true/false, optional)
VERBOSE_LOGGING=true
# Enable profiling (true/false, optional)
ENABLE_PROFILING=false
LOG_LEVEL=DEBUG
