# ============================================
# rAthena-AI-world - Unified Environment Configuration Example
# ============================================
# Copy this file to .env and fill in your actual values.
# DO NOT commit .env to version control!
# All secrets must use placeholders and be set securely in production.

# ============================================
# Database Configuration
# ============================================
# PostgreSQL database name (required)
POSTGRES_DB=rathena_ai
# PostgreSQL username (required)
POSTGRES_USER=rathena
# PostgreSQL password (required, set a strong password in production)
POSTGRES_PASSWORD=changeme_secure_password_here

# ============================================
# LLM Provider API Keys
# ============================================
# At least ONE provider must be configured for AI features.
# Recommended: OpenAI GPT-4 for best results.

# ============================================
# LLM Provider Configuration
# ============================================

# -------- OpenAI --------
# Required for OpenAI GPT usage
OPENAI_API_KEY=sk-your-openai-api-key-here      # Required if using OpenAI. Obtain from https://platform.openai.com/
OPENAI_MODEL=gpt-4                              # Required: Model name (e.g., gpt-4, gpt-3.5-turbo)
OPENAI_API_BASE=https://api.openai.com/v1        # Optional: Override OpenAI API base URL
OPENAI_API_VERSION=2023-07-01                    # Optional: API version (if needed)

# -------- Anthropic Claude --------
# Required for Anthropic Claude usage
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here   # Required if using Anthropic Claude
ANTHROPIC_MODEL=claude-3-opus                          # Required: Model name (e.g., claude-3-opus, claude-2.1)
ANTHROPIC_API_ENDPOINT=https://api.anthropic.com/v1    # Optional: Override Anthropic API endpoint

# -------- Google Gemini --------
# Required for Google Gemini usage
GOOGLE_API_KEY=your-google-api-key-here                # Required if using Google Gemini
GOOGLE_MODEL=gemini-pro                                # Required: Model name (e.g., gemini-pro, gemini-1.5-pro)
GOOGLE_API_ENDPOINT=https://generativelanguage.googleapis.com/v1beta  # Optional: Override Gemini API endpoint

# -------- Azure OpenAI --------
# Required for Azure OpenAI usage
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here    # Required if using Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/  # Required: Azure endpoint URL
AZURE_OPENAI_DEPLOYMENT=your-azure-deployment-name     # Required: Azure deployment/model name
AZURE_OPENAI_MODEL=gpt-4                              # Optional: Model name (if different from deployment)
AZURE_OPENAI_API_VERSION=2023-05-15                   # Optional: API version

# -------- DeepSeek --------
# Required for DeepSeek usage
DEEPSEEK_API_KEY=your-deepseek-api-key-here            # Required if using DeepSeek
DEEPSEEK_MODEL=deepseek-chat                           # Required: Model name (e.g., deepseek-chat, deepseek-coder)
DEEPSEEK_API_ENDPOINT=https://api.deepseek.com/v1      # Optional: Override DeepSeek API endpoint

# -------- General LLM Selection --------
DEFAULT_LLM_PROVIDER=openai                     # Default: openai (options: openai, anthropic, google, azure, deepseek)
DEFAULT_MODEL=gpt-4                             # Default model for LLM requests

# ============================================
# AI Service Configuration
# ============================================
# API key for internal AI service bridge (required, set a strong secret in production)
AI_SERVICE_API_KEY=your-secure-api-key-for-bridge-communication
# Logging level for AI service (optional, default: INFO)
LOG_LEVEL=INFO
# Maximum number of AI worker processes (optional, default: 4)
MAX_WORKERS=4
# Cache time-to-live in seconds (optional, default: 3600)
CACHE_TTL=3600

# ============================================
# Worker Pool / Multi-CPU / P2P Configuration
# ============================================
# Enable worker pool for multi-threaded operation (true/false, optional, default: true)
WORKER_POOL_ENABLED=true
# Number of worker threads (optional, overrides conf/worker_pool.conf)
WORKER_POOL_NUM_THREADS=4
# Minimum threads for dynamic scaling (optional)
WORKER_POOL_MIN_THREADS=2
# Maximum threads for dynamic scaling (optional)
WORKER_POOL_MAX_THREADS=8
# Assignment strategy for worker pool (optional: round_robin, least_loaded, hash)
WORKER_POOL_ASSIGNMENT_STRATEGY=least_loaded
# Enable Prometheus metrics (true/false, optional)
WORKER_POOL_METRICS_ENABLED=true
# Prometheus metrics listen address (optional, default: 0.0.0.0:9100)
WORKER_POOL_METRICS_ADDR=0.0.0.0:9100
# Structured logging level for worker pool (optional: debug, info, warn, error)
WORKER_POOL_LOG_LEVEL=info
# Log file path for worker pool (optional, empty for stdout)
WORKER_POOL_LOG_FILE=
# Verbose logging for worker pool (true/false, optional)
WORKER_POOL_VERBOSE=false
# Enable dynamic scaling of worker pool (true/false, optional)
WORKER_POOL_DYNAMIC_SCALING=true
# CPU affinity for worker pool (optional, comma-separated core indices, empty for auto)
WORKER_POOL_CPU_AFFINITY=

# ============================================
# Performance Tuning
# ============================================
# Dialogue generation temperature (0.0-1.0, higher = more creative, optional, default: 0.7)
DIALOGUE_TEMPERATURE=0.7
# Decision making temperature (optional, default: 0.5)
DECISION_TEMPERATURE=0.5
# Maximum tokens for LLM responses (optional, default: 500)
MAX_TOKENS=500
# Cache expiration in seconds (optional, default: 3600)
CACHE_EXPIRATION=3600

# ============================================
# Feature Flags
# ============================================
# Enable/disable subsystems (true/false, optional)
ENABLE_MEMORY_SYSTEM=true
ENABLE_PERSONALITY_SYSTEM=true
ENABLE_FACTION_SYSTEM=true
ENABLE_ECONOMY_AGENT=true
ENABLE_QUEST_GENERATION=true

# ============================================
# Development/Debug Settings
# ============================================
# Enable debug mode (true/false, optional)
DEBUG_MODE=false
# Enable verbose logging (true/false, optional)
VERBOSE_LOGGING=false
# Enable profiling (true/false, optional)
ENABLE_PROFILING=false
