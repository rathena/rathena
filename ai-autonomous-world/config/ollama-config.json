{
  "description": "Ollama Performance Tuning Configuration",
  "version": "1.0.0",
  "profiles": {
    "cpu_optimized": {
      "description": "Optimized for CPU-only inference with good balance",
      "num_ctx": 2048,
      "num_thread": 8,
      "num_gpu": 0,
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 40,
      "repeat_penalty": 1.1,
      "num_predict": 2000,
      "stop": ["</s>", "###"]
    },
    "gpu_optimized": {
      "description": "Optimized for GPU acceleration with maximum throughput",
      "num_ctx": 4096,
      "num_thread": 4,
      "num_gpu": 1,
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 40,
      "repeat_penalty": 1.1,
      "num_predict": 2000,
      "num_batch": 512,
      "stop": ["</s>", "###"]
    },
    "fast_inference": {
      "description": "Prioritize speed over quality for real-time responses",
      "num_ctx": 1024,
      "num_thread": 6,
      "num_gpu": 1,
      "temperature": 0.6,
      "top_p": 0.85,
      "top_k": 30,
      "repeat_penalty": 1.15,
      "num_predict": 1500,
      "stop": ["</s>", "###"]
    },
    "quality_optimized": {
      "description": "Prioritize quality over speed for important interactions",
      "num_ctx": 4096,
      "num_thread": 8,
      "num_gpu": 1,
      "temperature": 0.8,
      "top_p": 0.95,
      "top_k": 50,
      "repeat_penalty": 1.05,
      "num_predict": 3000,
      "stop": ["</s>", "###"]
    },
    "low_memory": {
      "description": "Reduced memory usage for systems with limited RAM",
      "num_ctx": 1024,
      "num_thread": 4,
      "num_gpu": 0,
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 40,
      "repeat_penalty": 1.1,
      "num_predict": 1500,
      "stop": ["</s>", "###"]
    }
  },
  "use_cases": {
    "npc_dialogue": {
      "description": "Conversational NPC interactions",
      "recommended_profile": "cpu_optimized",
      "temperature": 0.8,
      "num_predict": 300,
      "notes": "Higher temperature for more natural, varied responses"
    },
    "quest_generation": {
      "description": "Complex quest and story generation",
      "recommended_profile": "quality_optimized",
      "temperature": 0.75,
      "num_predict": 2000,
      "notes": "Balanced creativity with coherent narratives"
    },
    "decision_making": {
      "description": "NPC decision and action selection",
      "recommended_profile": "fast_inference",
      "temperature": 0.6,
      "num_predict": 200,
      "notes": "Lower temperature for more deterministic decisions"
    },
    "world_events": {
      "description": "Dynamic world event generation",
      "recommended_profile": "gpu_optimized",
      "temperature": 0.7,
      "num_predict": 1500,
      "notes": "Moderate creativity with consistent world logic"
    }
  },
  "model_specific": {
    "llama2:7b": {
      "recommended_contexts": {
        "cpu": 2048,
        "gpu": 4096
      },
      "optimal_threads": 6,
      "expected_latency_ms": {
        "cpu": "300-600",
        "gpu": "80-150"
      }
    },
    "llama2:13b": {
      "recommended_contexts": {
        "cpu": 2048,
        "gpu": 4096
      },
      "optimal_threads": 8,
      "expected_latency_ms": {
        "cpu": "200-400",
        "gpu": "60-120"
      }
    },
    "mistral:7b": {
      "recommended_contexts": {
        "cpu": 2048,
        "gpu": 4096
      },
      "optimal_threads": 6,
      "expected_latency_ms": {
        "cpu": "100-250",
        "gpu": "50-100"
      }
    },
    "codellama:13b": {
      "recommended_contexts": {
        "cpu": 2048,
        "gpu": 4096
      },
      "optimal_threads": 8,
      "expected_latency_ms": {
        "cpu": "200-400",
        "gpu": "60-120"
      }
    },
    "llama2:70b": {
      "recommended_contexts": {
        "gpu": 8192
      },
      "optimal_threads": 16,
      "expected_latency_ms": {
        "gpu": "100-200"
      },
      "notes": "Requires GPU with 40GB+ VRAM. Not recommended for CPU."
    }
  },
  "hardware_recommendations": {
    "minimum": {
      "cpu": "4 cores",
      "ram": "8GB",
      "model": "mistral:7b",
      "expected_performance": "300-600ms per request"
    },
    "recommended": {
      "cpu": "8 cores",
      "ram": "16GB",
      "model": "llama2:13b",
      "expected_performance": "200-400ms per request"
    },
    "optimal": {
      "cpu": "8+ cores",
      "gpu": "NVIDIA GPU with 16GB+ VRAM (RTX 4070 or better)",
      "ram": "32GB",
      "model": "llama2:13b or llama2:70b",
      "expected_performance": "60-120ms per request"
    },
    "production": {
      "cpu": "16+ cores",
      "gpu": "NVIDIA A100 (40GB) or H100",
      "ram": "64GB+",
      "model": "llama2:70b",
      "expected_performance": "50-100ms per request",
      "concurrent_requests": "10+"
    }
  },
  "troubleshooting": {
    "high_latency": {
      "symptoms": "Responses taking >1000ms",
      "solutions": [
        "Switch to smaller model (mistral:7b)",
        "Reduce num_ctx to 1024",
        "Reduce num_predict to 1500",
        "Increase num_thread if CPU has more cores",
        "Enable GPU acceleration if available",
        "Close other applications to free RAM"
      ]
    },
    "out_of_memory": {
      "symptoms": "Ollama crashes or refuses to load model",
      "solutions": [
        "Switch to smaller model",
        "Use quantized model (e.g., llama2:13b-q4_0)",
        "Reduce num_ctx to 1024",
        "Close other applications",
        "Restart Ollama service",
        "Use CPU-only mode (num_gpu=0)"
      ]
    },
    "inconsistent_responses": {
      "symptoms": "Responses vary wildly in quality",
      "solutions": [
        "Lower temperature to 0.6-0.7",
        "Increase repeat_penalty to 1.15",
        "Use more specific system prompts",
        "Increase num_predict if responses are cut off",
        "Check for adequate num_ctx (2048+)"
      ]
    },
    "connection_errors": {
      "symptoms": "Cannot connect to Ollama",
      "solutions": [
        "Verify Ollama is running: `ollama serve`",
        "Check port 11434 is available: `netstat -an | grep 11434`",
        "Check firewall settings",
        "Verify OLLAMA_BASE_URL in .env",
        "Restart Ollama service"
      ]
    }
  },
  "best_practices": {
    "development": {
      "model": "mistral:7b",
      "profile": "fast_inference",
      "notes": "Use fast model for rapid iteration"
    },
    "staging": {
      "model": "llama2:13b",
      "profile": "cpu_optimized",
      "notes": "Test with production-like model"
    },
    "production": {
      "model": "llama2:13b",
      "profile": "gpu_optimized",
      "notes": "Use GPU for best performance"
    }
  },
  "monitoring": {
    "key_metrics": [
      "response_latency_ms",
      "tokens_per_second",
      "memory_usage_mb",
      "concurrent_requests",
      "error_rate"
    ],
    "alerts": {
      "latency_p95_threshold_ms": 800,
      "memory_usage_threshold_percent": 85,
      "error_rate_threshold_percent": 5
    }
  }
}