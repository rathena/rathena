# AI Service Environment Configuration
# Copy this file to .env and fill in your actual values
#
# IMPORTANT: This file contains placeholder values. You MUST update:
# - At least one LLM provider API key (Azure OpenAI recommended)
# - PostgreSQL password (if different from default)
# - DragonflyDB password (if using authentication)

# ============================================================================
# SERVICE CONFIGURATION
# ============================================================================
SERVICE_HOST=0.0.0.0
SERVICE_PORT=8000
SERVICE_ENV=development
LOG_LEVEL=DEBUG

# ============================================================================
# DRAGONFLY DB (Redis-compatible) - For caching and real-time state
# ============================================================================
# DragonflyDB provides high-speed caching for:
# - NPC state and decision caching
# - LLM response caching
# - World state caching
# - Real-time event queues (Redis Pub/Sub)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_MAX_CONNECTIONS=50

# ============================================================================
# POSTGRESQL 17 - For persistent memory storage
# ============================================================================
# PostgreSQL stores long-term data:
# - NPC memories and relationships
# - Faction data and player reputation
# - Quest history
# - Economic history
#
# Required extensions: pgvector, TimescaleDB, Apache AGE
# Run migrations: psql -h localhost -U ai_world_user -d ai_world_memory -f ai-service/migrations/001_create_factions_table.sql
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=ai_world_memory
POSTGRES_USER=ai_world_user
POSTGRES_PASSWORD=ai_world_pass_2025
POSTGRES_POOL_SIZE=10
POSTGRES_MAX_OVERFLOW=20
POSTGRES_ECHO_SQL=false

# Database connection retry configuration
DB_CONNECTION_MAX_RETRIES=5
DB_CONNECTION_RETRY_DELAY=2.0

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
#
# The system supports multiple LLM providers with automatic fallback.
# Configure at least ONE provider below. Azure OpenAI is recommended for production.
#
# All LLM configurations are loaded from YAML config files in config/ directory,
# which use environment variable placeholders in the format: ${VARIABLE_NAME}
#
# Supported providers:
# - azure_openai (Recommended - enterprise-grade, reliable)
# - openai (Good alternative, simpler setup)
# - anthropic (Claude - excellent for dialogue)
# - google (Gemini - cost-effective)
# - deepseek (Very cost-effective)
# - ollama (Local development, no API costs)

# Primary LLM Provider (azure_openai, openai, ollama, deepseek, gemini, claude)
PRIMARY_LLM_PROVIDER=azure_openai

# ============================================================================
# AZURE OPENAI (Recommended for production)
# ============================================================================
# Azure OpenAI provides enterprise-grade reliability and compliance.
# Get your credentials from: https://portal.azure.com
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002

# ============================================================================
# OPENAI (Alternative to Azure)
# ============================================================================
# Standard OpenAI API - simpler setup than Azure.
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ============================================================================
# OLLAMA (Local development - no API costs)
# ============================================================================
# Run local LLMs on your machine. Install from: https://ollama.ai
# Start Ollama: ollama serve
# Pull models: ollama pull llama2
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# ============================================================================
# DEEPSEEK (Cost-effective alternative)
# ============================================================================
# DeepSeek offers competitive pricing and good performance.
# Get your API key from: https://platform.deepseek.com
DEEPSEEK_API_KEY=your-deepseek-api-key-here
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TEMPERATURE=0.7
DEEPSEEK_MAX_TOKENS=2000

# ============================================================================
# GOOGLE GEMINI
# ============================================================================
# Google's Gemini models offer good performance at competitive pricing.
# Get your API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-pro

# ============================================================================
# ANTHROPIC CLAUDE
# ============================================================================
# Claude excels at dialogue and creative writing tasks.
# Get your API key from: https://console.anthropic.com
ANTHROPIC_API_KEY=your-anthropic-api-key-here
CLAUDE_MODEL=claude-3-opus-20240229

# ============================================================================
# LLM OPTIMIZATION
# ============================================================================
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600
LLM_BATCH_SIZE=10
LLM_RATE_LIMIT_RPM=60
LLM_TIMEOUT=30

# ============================================================================
# CREWAI CONFIGURATION
# ============================================================================
CREWAI_VERBOSE=true
CREWAI_MAX_ITERATIONS=10

# ============================================================================
# RATHENA BRIDGE CONFIGURATION
# ============================================================================
RATHENA_BRIDGE_URL=http://localhost:8080
RATHENA_BRIDGE_TIMEOUT=10

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
MAX_CONCURRENT_NPCS=100
NPC_DECISION_INTERVAL=10
WORLD_UPDATE_INTERVAL=60

# ============================================================================
# MONITORING
# ============================================================================
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=9090

# ============================================================================
# SECURITY AND RATE LIMITING
# ============================================================================

# API Key Authentication
API_KEY=your-secret-api-key-here
API_KEY_HEADER=X-API-Key
API_KEY_REQUIRED=false

# Rate Limiting (DragonflyDB-based distributed rate limiting)
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=60

# Request Size Limits
MAX_REQUEST_SIZE=10485760

# CORS Origins (comma-separated list)
CORS_ORIGINS=http://localhost:8888,http://127.0.0.1:8888

# ============================================================================
# DEVELOPMENT
# ============================================================================
DEBUG=true
RELOAD=true
