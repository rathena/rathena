# AI Service Environment Configuration
# Copy this file to .env and fill in your actual values
#
# IMPORTANT: This file contains placeholder values. You MUST update:
# - At least one LLM provider API key (Azure OpenAI recommended)
# - PostgreSQL password (if different from default)
# - DragonflyDB password (if using authentication)

# ============================================================================
# SERVICE CONFIGURATION
# ============================================================================
SERVICE_HOST=0.0.0.0
SERVICE_PORT=8000
SERVICE_ENV=development
LOG_LEVEL=DEBUG

# ============================================================================
# DRAGONFLY DB (Redis-compatible) - For caching and real-time state
# ============================================================================
# DragonflyDB provides high-speed caching for:
# - NPC state and decision caching
# - LLM response caching
# - World state caching
# - Real-time event queues (Redis Pub/Sub)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=CHANGE_ME_GENERATE_STRONG_PASSWORD_32_CHARS
REDIS_DB=0
REDIS_MAX_CONNECTIONS=50

# ============================================================================
# POSTGRESQL 17 - For persistent memory storage
# ============================================================================
# PostgreSQL stores long-term data:
# - NPC memories and relationships
# - Faction data and player reputation
# - Quest history
# - Economic history
#
# Required extensions: pgvector, TimescaleDB, Apache AGE
# Run migrations: psql -h localhost -U ai_world_user -d ai_world_memory -f ai-service/migrations/001_create_factions_table.sql
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=ai_world_memory
POSTGRES_USER=ai_world_user
POSTGRES_PASSWORD=CHANGE_ME_GENERATE_STRONG_PASSWORD_32_CHARS
POSTGRES_POOL_SIZE=10
POSTGRES_MAX_OVERFLOW=20
POSTGRES_ECHO_SQL=false

# Database connection retry configuration
DB_CONNECTION_MAX_RETRIES=5
DB_CONNECTION_RETRY_DELAY=2.0

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
#
# The system supports multiple LLM providers with automatic fallback.
# Ollama is now the DEFAULT provider for self-contained, local operation.
#
# All LLM configurations are loaded from YAML config files in config/ directory,
# which use environment variable placeholders in the format: ${VARIABLE_NAME}
#
# Supported providers:
# - ollama (DEFAULT - Local LLM, no API costs, self-contained)
# - azure_openai (Enterprise-grade, reliable, requires API key)
# - openai (Good alternative, simpler setup, requires API key)
# - anthropic (Claude - excellent for dialogue, requires API key)
# - google (Gemini - cost-effective, requires API key)
# - deepseek (Very cost-effective, requires API key)

# Primary LLM Provider (ollama recommended for self-contained deployment)
DEFAULT_LLM_PROVIDER=ollama

# ============================================================================
# AZURE OPENAI (Optional fallback provider)
# ============================================================================
# Azure OpenAI provides enterprise-grade reliability and compliance.
# Only needed if you want to use Azure as fallback/alternative to Ollama.
# Get your credentials from: https://portal.azure.com
# Leave commented/empty if using Ollama only.
# AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_VERSION=2024-02-15-preview
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002

# ============================================================================
# OPENAI (Optional fallback provider)
# ============================================================================
# Standard OpenAI API - simpler setup than Azure.
# Only needed if you want to use OpenAI as fallback/alternative to Ollama.
# Get your API key from: https://platform.openai.com/api-keys
# Leave commented/empty if using Ollama only.
# OPENAI_API_KEY=your-openai-api-key-here
# OPENAI_MODEL=gpt-4-turbo-preview
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ============================================================================
# OLLAMA (DEFAULT - Local LLM, no API costs)
# ============================================================================
# Ollama runs local LLMs on your machine with no external dependencies.
#
# Installation:
# 1. Install Ollama: curl https://ollama.ai/install.sh | sh
# 2. Start service: ollama serve (runs on port 11434)
# 3. Pull model: ollama pull llama2:13b
#
# Recommended models (balance quality/speed/memory):
# - llama2:13b (DEFAULT - 13GB RAM, 200-400ms, good balance)
# - mistral:7b (7GB RAM, 100-250ms, faster inference)
# - codellama:13b (13GB RAM, better for quests/logic)
# - llama2:70b (40GB+ RAM, highest quality, requires GPU)
#
# Performance: 3-5x faster than external APIs (100-500ms vs 800-2500ms)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2:13b
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2000
OLLAMA_TIMEOUT=120

# ============================================================================
# DEEPSEEK (Cost-effective alternative)
# ============================================================================
# DeepSeek offers competitive pricing and good performance.
# Get your API key from: https://platform.deepseek.com
DEEPSEEK_API_KEY=your-deepseek-api-key-here
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TEMPERATURE=0.7
DEEPSEEK_MAX_TOKENS=2000

# ============================================================================
# GOOGLE GEMINI
# ============================================================================
# Google's Gemini models offer good performance at competitive pricing.
# Get your API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-pro

# ============================================================================
# ANTHROPIC CLAUDE
# ============================================================================
# Claude excels at dialogue and creative writing tasks.
# Get your API key from: https://console.anthropic.com
ANTHROPIC_API_KEY=your-anthropic-api-key-here
CLAUDE_MODEL=claude-3-opus-20240229

# ============================================================================
# LLM OPTIMIZATION
# ============================================================================
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600
LLM_BATCH_SIZE=10
LLM_RATE_LIMIT_RPM=60
LLM_TIMEOUT=30

# ============================================================================
# CREWAI CONFIGURATION
# ============================================================================
CREWAI_VERBOSE=true
CREWAI_MAX_ITERATIONS=10

# ============================================================================
# RATHENA BRIDGE CONFIGURATION
# ============================================================================

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
MAX_CONCURRENT_NPCS=100
NPC_DECISION_INTERVAL=10
WORLD_UPDATE_INTERVAL=60

# ============================================================================
# MONITORING
# ============================================================================
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=9090

# ============================================================================
# SECURITY AND RATE LIMITING
# ============================================================================

# API Key Authentication (CRITICAL: Enable for production)
API_KEY=CHANGE_ME_RUN_generate-secure-config.py
API_KEY_HEADER=X-API-Key
API_KEY_REQUIRED=true

# Rate Limiting (DragonflyDB-based distributed rate limiting)
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_PERIOD=60

# Request Size Limits (SEC-006: Prevents DoS)
MAX_REQUEST_SIZE=1048576

# CORS Origins (comma-separated list - RESTRICT IN PRODUCTION)
CORS_ORIGINS=http://localhost:8888,http://127.0.0.1:8888

# SSL/TLS Configuration (SEC-003: HTTPS Enforcement)
SSL_ENABLED=false
SSL_KEYFILE=certs/key.pem
SSL_CERTFILE=certs/cert.pem

# Security Headers (SEC-005: XSS/Clickjacking Protection)
SECURITY_HEADERS_ENABLED=true

# JWT Secret for P2P Coordinator (SEC-007)
JWT_SECRET=CHANGE_ME_RUN_generate-secure-config.py_64_CHARS
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=10080

# ============================================================================
# DEVELOPMENT
# ============================================================================
DEBUG=true
RELOAD=true
